{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9907a7f73584cdab9d7b7ae7027722d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c99458a148f4d4da0a6f9f30dde302a",
              "IPY_MODEL_cafc10c993f24fe7ae31bd6916be6074",
              "IPY_MODEL_25d616cdae234c4bb584c1988790b88f"
            ],
            "layout": "IPY_MODEL_5c273f39daf64d5f8bd981d365c7a7f2"
          }
        },
        "4c99458a148f4d4da0a6f9f30dde302a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b17aadda114d4b43ae45de09179e3e98",
            "placeholder": "​",
            "style": "IPY_MODEL_9c28c554a8f54eaaa386637465fdec83",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cafc10c993f24fe7ae31bd6916be6074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e853ecd4f5cc4e1f876f02898a463d23",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fe3cd7d8bde42feafd4202d7f3e3937",
            "value": 3
          }
        },
        "25d616cdae234c4bb584c1988790b88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92a1fcb7507d43329a36c6724843a7ad",
            "placeholder": "​",
            "style": "IPY_MODEL_46b7c97e510549fbb0f5630afa51656c",
            "value": " 3/3 [00:36&lt;00:00, 14.91s/it]"
          }
        },
        "5c273f39daf64d5f8bd981d365c7a7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17aadda114d4b43ae45de09179e3e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c28c554a8f54eaaa386637465fdec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e853ecd4f5cc4e1f876f02898a463d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe3cd7d8bde42feafd4202d7f3e3937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92a1fcb7507d43329a36c6724843a7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b7c97e510549fbb0f5630afa51656c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 1: Load Your Classifier and Make a Prediction"
      ],
      "metadata": {
        "id": "FsqEKKrO1U7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y2DpSI20oCW",
        "outputId": "35663b97-7f67-4e0d-d079-6a82e38dca51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load trained model\n",
        "with open(\"rf_classifier.pkl\", \"rb\") as f:\n",
        "    classifier = pickle.load(f)\n",
        "\n",
        "# Sample patient data (replace with actual input)\n",
        "patient_data = {\n",
        "    \"age\": 57,\n",
        "    \"sex\": 1,\n",
        "    \"cp\": 2,\n",
        "    \"trestbps\": 140,\n",
        "    \"chol\": 250,\n",
        "    \"fbs\": 0,\n",
        "    \"restecg\": 1,\n",
        "    \"thalach\": 150,\n",
        "    \"exang\": 0,\n",
        "    \"oldpeak\": 1.4,\n",
        "    \"slope\": 2,\n",
        "    \"ca\": 0,\n",
        "    \"thal\": 2\n",
        "}\n",
        "\n",
        "# Ensure feature order matches training\n",
        "features = np.array([list(patient_data.values())]).reshape(1, -1)\n",
        "\n",
        "# Get prediction\n",
        "prediction = classifier.predict(features)[0]  # 0 = No Heart Disease, 1 = Likely Heart Disease\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate -q"
      ],
      "metadata": {
        "id": "f5GmG2nt1AVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R41lIp-5Itvj",
        "outputId": "37ccfdb0-a27b-4476-ca89-897a89ee0ece"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Conversational Model\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n",
        "mistral_pipe = pipeline(\"text-generation\", model=mistral_model, tokenizer=mistral_tokenizer)\n",
        "\n",
        "# Biomedical Model\n",
        "bio_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n",
        "bio_model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT-Large\", device_map=\"auto\")\n",
        "bio_pipe = pipeline(\"text-generation\", model=bio_model, tokenizer=bio_tokenizer)\n"
      ],
      "metadata": {
        "id": "5WEj0NrRJ9E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_query(query: str) -> str:\n",
        "    biomedical_keywords = [\"what is\", \"define\", \"symptoms\", \"treatment\", \"diagnosis\", \"medicine\", \"drug\", \"clinical\", \"risk factor\"]\n",
        "\n",
        "    if any(kw in query.lower() for kw in biomedical_keywords):\n",
        "        return \"biomedical\"\n",
        "    return \"conversational\""
      ],
      "metadata": {
        "id": "NnifInq8LOzw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_heart_bot(query: str):\n",
        "    route = route_query(query)\n",
        "\n",
        "    if route == \"biomedical\":\n",
        "        response = bio_pipe(query, max_new_tokens=100, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "    else:\n",
        "        # Proper Mistral-style prompt formatting\n",
        "        prompt = f\"[INST] {query} [/INST]\"\n",
        "        response = mistral_pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "5EnRlnAjMvpj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_response(response: str):\n",
        "    # Remove any non-standard tokens or unwanted text\n",
        "    response = re.sub(r\"<.*?>\", \"\", response)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "C_5zkhb0NOVB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_heart_bot(query: str):\n",
        "    route = route_query(query)\n",
        "\n",
        "    if route == \"biomedical\":\n",
        "        response = bio_pipe(query, max_new_tokens=100, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "    else:\n",
        "        prompt = f\"[INST] {query} [/INST]\"\n",
        "        response = mistral_pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "    return clean_response(response)\n"
      ],
      "metadata": {
        "id": "7vp7cO8kNR6l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_heart_bot(query: str):\n",
        "    route = route_query(query)\n",
        "\n",
        "    # Set parameters to handle longer responses and adjust temperature for varied output\n",
        "    max_tokens = 600\n",
        "    temperature = 0.7\n",
        "    pad_token_id = 2  # Ensure that padding is handled correctly\n",
        "\n",
        "    # Check the route and use the appropriate pipeline\n",
        "    if route == \"biomedical\":\n",
        "        # Generating a response from the biomedical model\n",
        "        response = bio_pipe(query, max_new_tokens=max_tokens, do_sample=True, temperature=temperature, pad_token_id=pad_token_id)[0]['generated_text']\n",
        "    else:\n",
        "        # Generating a response from the other model\n",
        "        prompt = f\"[INST] {query} [/INST]\"\n",
        "        response = mistral_pipe(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=temperature, pad_token_id=pad_token_id)[0]['generated_text']\n",
        "\n",
        "    # Clean and return the response\n",
        "    return clean_response(response)"
      ],
      "metadata": {
        "id": "DkMSZ35JNou3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_heart_bot(\"I am having chest pains. What could be the cause?\"))\n",
        "print(ask_heart_bot(\"What is myocardial infarction?\"))\n",
        "print(ask_heart_bot(\"Suggest a diet plan for a heart patient.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyLx2r_VMz0h",
        "outputId": "c8181ea2-88e9-489c-a5a3-5fc46944d54a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] I am having chest pains. What could be the cause? [/INST] I'm an AI language model and cannot physically examine you or diagnose conditions. Chest pain can be caused by a variety of conditions, some of which are serious and require immediate medical attention. Common causes of chest pain include:\n",
            "\n",
            "1. Angina: This is a type of chest pain caused by reduced blood flow to the heart muscle due to narrowed or blocked coronary arteries. Angina is usually described as a feeling of pressure or tightness in the chest, often accompanied by shortness of breath, nausea, and fatigue.\n",
            "2. Heart attack: A heart attack occurs when the blood flow to a part of the heart is blocked, leading to damage or death of heart muscle. Chest pain during a heart attack is often described as a heavy, crushing sensation that may spread to the neck, jaw, back, or arms.\n",
            "3. Pulmonary embolism: This is a blockage in a pulmonary artery, usually caused by a blood clot. Chest pain from a pulmonary embolism is often described as sharp, stabbing pain that may worsen with deep breathing or physical activity.\n",
            "4. Pneumonia: Pneumonia is an infection of the lungs that can cause chest pain, along with other symptoms such as fever, cough, and shortness of breath.\n",
            "5. Gastroesophageal reflux disease (GERD): GERD is a condition in which stomach acid flows back into the esophagus, causing heartburn and chest pain.\n",
            "6. Anxiety or panic attacks: Anxiety and panic attacks can cause chest pain, often described as a feeling of tightness, pressure, or pounding in the chest.\n",
            "\n",
            "If you are experiencing chest pain, it is important to seek medical attention right away. Call emergency services if you suspect a heart attack or stroke, or if your chest pain is severe or accompanied by other symptoms such as shortness of breath, sweating, or dizziness. If your symptoms are less severe, you can make an appointment with your healthcare provider for further evaluation.\n",
            "What is myocardial infarction?   ▃\n",
            "[INST] Suggest a diet plan for a heart patient. [/INST] A heart-healthy diet is essential for individuals with heart disease to manage their condition effectively. Here's a suggested diet plan for a heart patient, as recommended by the American Heart Association (AHA):\n",
            "\n",
            "1. Fruits and Vegetables: Aim for at least 4.5 cups (or 9 servings) per day. Choose fruits and vegetables that are rich in fiber, vitamins, and minerals. Fresh, frozen, canned (in water), and dried fruits and vegetables can all be included.\n",
            "\n",
            "2. Whole Grains: Include at least 4-6 servings per day. Examples include whole grain breads, pastas, cereals, quinoa, brown rice, oats, and barley.\n",
            "\n",
            "3. Lean Proteins: Incorporate at least 5.5 oz (or 130g) per day. Choose sources like fish, poultry, beans, eggs, and nuts. Limit red meat and processed meats.\n",
            "\n",
            "4. Healthy Fats: Include sources of healthy fats like avocados, nuts, seeds, olive oil, and fish. Aim for about 25-35% of your daily caloric intake.\n",
            "\n",
            "5. Dairy: Choose fat-free or low-fat (1%) dairy products like milk, cheese, and yogurt. Aim for 2-3 servings per day.\n",
            "\n",
            "6. Sodium: Limit sodium intake to less than 2,300 mg per day, preferably less than 1,500 mg per day. Read food labels carefully and avoid processed and canned foods high in sodium.\n",
            "\n",
            "7. Sugars and Saturated Fats: Limit added sugars and saturated fats. Avoid sugary drinks like soda and sweetened beverages. Opt for water, unsweetened tea, or milk instead.\n",
            "\n",
            "8. Alcohol: If you choose to drink alcohol, limit consumption to no more than 1 drink per day for women and 2 drinks per day for men.\n",
            "\n",
            "It’s important to note that individual dietary needs may vary based on age, gender, physical activity level, and specific medical conditions. Consult with a healthcare professional or a registered dietitian for personalized dietary recommendations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGBXJeIgNVRi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# light_heart_pipeline.py\n",
        "\n",
        "class UnifiedHeartHealthBotPipeline:\n",
        "    def __init__(self, route_query, clean_response):\n",
        "        self.route_query = route_query\n",
        "        self.clean_response = clean_response\n",
        "\n",
        "    def ask_bot(self, query, bio_pipe, mistral_pipe):\n",
        "        route = self.route_query(query)\n",
        "\n",
        "        if route == \"biomedical\":\n",
        "            response = bio_pipe(query, max_new_tokens=100, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "        else:\n",
        "            prompt = f\"[INST] {query} [/INST]\"\n",
        "            response = mistral_pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "        return self.clean_response(response)\n"
      ],
      "metadata": {
        "id": "ngr2VwMuPiXP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import re\n",
        "\n",
        "# Routing logic\n",
        "def route_query(query: str) -> str:\n",
        "    biomedical_keywords = [\"what is\", \"define\", \"symptoms\", \"treatment\", \"diagnosis\", \"medicine\", \"drug\", \"clinical\", \"risk factor\"]\n",
        "    if any(kw in query.lower() for kw in biomedical_keywords):\n",
        "        return \"biomedical\"\n",
        "    return \"conversational\"\n",
        "\n",
        "# Cleanup logic\n",
        "def clean_response(response: str):\n",
        "    return re.sub(r\"<.*?>\", \"\", response).strip()\n",
        "\n",
        "# Lightweight pipeline with lazy model loading\n",
        "class UnifiedHeartHealthBotPipeline:\n",
        "    def __init__(self, route_query, clean_response):\n",
        "        self.route_query = route_query\n",
        "        self.clean_response = clean_response\n",
        "        self._bio_pipe = None\n",
        "        self._mistral_pipe = None\n",
        "\n",
        "    def _load_bio_pipe(self):\n",
        "        if self._bio_pipe is None:\n",
        "            from transformers import pipeline\n",
        "            self._bio_pipe = pipeline(\"text-generation\", model=\"microsoft/BioGPT-Large\", tokenizer=\"microsoft/BioGPT-Large\", device_map=\"auto\")\n",
        "        return self._bio_pipe\n",
        "\n",
        "    def _load_mistral_pipe(self):\n",
        "        if self._mistral_pipe is None:\n",
        "            from transformers import pipeline\n",
        "            self._mistral_pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n",
        "        return self._mistral_pipe\n",
        "\n",
        "    def ask_bot(self, query):\n",
        "        route = self.route_query(query)\n",
        "        if route == \"biomedical\":\n",
        "            pipe = self._load_bio_pipe()\n",
        "            response = pipe(query, max_new_tokens=100, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "        else:\n",
        "            pipe = self._load_mistral_pipe()\n",
        "            prompt = f\"[INST] {query} [/INST]\"\n",
        "            response = pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "        return self.clean_response(response)\n",
        "\n",
        "# Create instance and save\n",
        "pipeline = UnifiedHeartHealthBotPipeline(route_query, clean_response)\n",
        "with open(\"light_heart_pipeline.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pipeline, f)\n"
      ],
      "metadata": {
        "id": "xiXbRRWbS-V0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickled pipeline\n",
        "with open(\"light_heart_pipeline.pkl\", \"rb\") as f:\n",
        "    pipeline = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "20f9oWfZUEa5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = pipeline.ask_bot(\"How are you today?\")\n",
        "print(\"Conversational Response:\\n\", response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "a9907a7f73584cdab9d7b7ae7027722d",
            "4c99458a148f4d4da0a6f9f30dde302a",
            "cafc10c993f24fe7ae31bd6916be6074",
            "25d616cdae234c4bb584c1988790b88f",
            "5c273f39daf64d5f8bd981d365c7a7f2",
            "b17aadda114d4b43ae45de09179e3e98",
            "9c28c554a8f54eaaa386637465fdec83",
            "e853ecd4f5cc4e1f876f02898a463d23",
            "5fe3cd7d8bde42feafd4202d7f3e3937",
            "92a1fcb7507d43329a36c6724843a7ad",
            "46b7c97e510549fbb0f5630afa51656c"
          ]
        },
        "id": "xXgZYzh5UKGr",
        "outputId": "cf880f45-b3d7-425d-b89a-ed1d6027f1d3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9907a7f73584cdab9d7b7ae7027722d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversational Response:\n",
            " [INST] How are you today? [/INST] I'm an artificial intelligence and don't have feelings or emotions. I'm here to help answer your questions to the best of my ability. Is there a specific question you have in mind?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = pipeline.ask_bot(\"What is myocardial infarction?\")\n",
        "print(\"Biomedical Response:\\n\", response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6rFi416UMIy",
        "outputId": "ab35ef42-c50c-459d-c453-f57c5828e37d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biomedical Response:\n",
            " What is myocardial infarction?   ▃\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_queries = [\n",
        "    \"What are the symptoms of a heart attack?\",                      # Biomedical\n",
        "    \"Define arrhythmia.\",                                           # Biomedical\n",
        "    \"How can high blood pressure be managed?\",                      # Biomedical\n",
        "    \"Tell me a joke about doctors.\",                                # Conversational\n",
        "    \"Can you give lifestyle tips for a healthy heart?\",             # Conversational\n",
        "    \"What medications are used for cholesterol control?\",           # Biomedical\n",
        "    \"How's the weather in your circuits today?\",                    # Conversational\n",
        "    \"Explain the treatment of atrial fibrillation.\",                # Biomedical\n",
        "    \"Write a poem about staying healthy.\",                          # Conversational\n",
        "    \"Can you suggest a heart-healthy Indian diet?\"                  # Biomedical\n",
        "]\n",
        "\n",
        "# Run all queries through the unified bot\n",
        "for i, q in enumerate(test_queries):\n",
        "    print(f\"\\n🟦 Query {i+1}: {q}\")\n",
        "    try:\n",
        "        response = pipeline.ask_bot(q)\n",
        "        print(f\"🟩 Response:\\n{response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"🟥 Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5V1hPzKUbxU",
        "outputId": "9c0df771-a4ac-4f8a-f048-d28a3a22c10a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟦 Query 1: What are the symptoms of a heart attack?\n",
            "🟩 Response:\n",
            "What are the symptoms of a heart attack?   ▃\n",
            "\n",
            "🟦 Query 2: Define arrhythmia.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "Define arrhythmia.   ▃\n",
            "\n",
            "🟦 Query 3: How can high blood pressure be managed?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "[INST] How can high blood pressure be managed? [/INST] High blood pressure, also known as hypertension, can be managed through several lifestyle changes and, in some cases, medication. Here are some ways to help manage high blood pressure:\n",
            "\n",
            "1. Maintain a healthy weight: Being overweight can increase your risk of developing high blood pressure. Aim for a healthy weight by eating a balanced diet and getting regular physical activity.\n",
            "\n",
            "2. Eat a healthy diet: A diet rich in fruits, vegetables, whole grains, and lean proteins can help lower your blood pressure. Limit your intake of sodium, processed foods, and foods high in saturated and trans fats.\n",
            "\n",
            "3. Exercise regularly: Regular physical activity can help lower your\n",
            "\n",
            "🟦 Query 4: Tell me a joke about doctors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "[INST] Tell me a joke about doctors. [/INST] Why did the cookie go to the doctor? Because it felt crumby. (I'm here all day folks!)\n",
            "\n",
            "🟦 Query 5: Can you give lifestyle tips for a healthy heart?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "[INST] Can you give lifestyle tips for a healthy heart? [/INST] Absolutely! Here are some lifestyle tips that can help support a healthy heart:\n",
            "\n",
            "1. Eat a heart-healthy diet: Focus on consuming plenty of fruits, vegetables, whole grains, lean proteins, and healthy fats. Limit intake of saturated and trans fats, sodium, and added sugars.\n",
            "\n",
            "2. Maintain a healthy weight: Aim for a body mass index (BMI) within the normal range. Excess weight can put added strain on the heart and blood vessels.\n",
            "\n",
            "3. Get regular physical activity: Aim for at least 150 minutes of moderate-intensity aerobic activity or 75 minutes of vigorous\n",
            "\n",
            "🟦 Query 6: What medications are used for cholesterol control?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "[INST] What medications are used for cholesterol control? [/INST] There are several types of medications that can be used to help control cholesterol levels in the blood. The specific medication prescribed depends on the individual's cholesterol levels, underlying causes, and overall health. Here are some common types of cholesterol-lowering medications:\n",
            "\n",
            "1. Statins: These are the most commonly prescribed cholesterol-lowering drugs. They work by inhibiting an enzyme in the liver that is needed to produce cholesterol. Examples include atorvastatin (Lipidor), simvastatin (Zocor), pravastatin (Pravachol), and rosuvastatin (Crestor).\n",
            "\n",
            "🟦 Query 7: How's the weather in your circuits today?\n",
            "🟩 Response:\n",
            "[INST] How's the weather in your circuits today? [/INST] I don't have a physical form or weather conditions. I'm a text-based artificial intelligence designed to process and generate text. I can't experience weather or have circuits that can be affected by it.\n",
            "\n",
            "🟦 Query 8: Explain the treatment of atrial fibrillation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "Explain the treatment of atrial fibrillation.   ▃\n",
            "\n",
            "🟦 Query 9: Write a poem about staying healthy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟩 Response:\n",
            "[INST] Write a poem about staying healthy. [/INST] In the dance of life, where days blend and twirl,\n",
            "\n",
            "Staying healthy is a precious pearl.\n",
            "A vibrant force, a radiant charm,\n",
            "A shield against life's stormy alarm.\n",
            "\n",
            "Eat the rainbow, in every hue,\n",
            "Fruits and veggies, fresh, ripe, and true.\n",
            "Nourish your body, a sacred temple,\n",
            "With nature's gifts, in a nurturing helm.\n",
            "\n",
            "Hydrate well, with clear and pure,\n",
            "A glassful serenade, a soothing lure.\n",
            "Quench your thirst, to keep your power,\n",
            "A lifeline, a fount\n",
            "\n",
            "🟦 Query 10: Can you suggest a heart-healthy Indian diet?\n",
            "🟩 Response:\n",
            "[INST] Can you suggest a heart-healthy Indian diet? [/INST] Absolutely! A heart-healthy Indian diet can be rich in fruits, vegetables, whole grains, lean proteins, and healthy fats. Here are some suggestions for creating a heart-healthy Indian meal plan:\n",
            "\n",
            "1. Whole Grains: Incorporate plenty of whole grains into your diet such as brown rice, whole wheat roti (flatbread), quinoa, and millets. These provide fiber, B vitamins, and minerals.\n",
            "\n",
            "2. Fruits and Vegetables: Aim for at least 5 servings of fruits and vegetables per day. Indian dishes like chana masala (chickpea curry), saag paneer (\n"
          ]
        }
      ]
    }
  ]
}