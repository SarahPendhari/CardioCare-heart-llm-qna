{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNQ0+HwZkgqy32fwZhd4ahm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahPendhari/CardioCare-heart-llm-qna/blob/main/LLMfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set CUDA environment variable for better error reporting\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets accelerate -q\n",
        "\n",
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
        "print(f\"Original dataset size: Train={len(dataset['train'])}, Test={len(dataset['test'])}\")\n",
        "\n",
        "# Function to identify heart/cardiovascular related questions\n",
        "def is_heart_related(example):\n",
        "    \"\"\"Check if a question is related to cardiology/heart\"\"\"\n",
        "    heart_keywords = [\n",
        "        'heart', 'cardiac', 'cardio', 'cardiovascular', 'myocardial', 'coronary',\n",
        "        'arrhythmia', 'atrial', 'ventricular', 'aorta', 'aortic', 'angina',\n",
        "        'cardiomyopathy', 'pericardial', 'pericardium', 'endocarditis',\n",
        "        'atherosclerosis', 'ecg', 'electrocardiogram', 'ekg', 'tachycardia',\n",
        "        'bradycardia', 'fibrillation', 'palpitation', 'hypertension', 'hypertensive',\n",
        "        'thrombosis', 'embolism', 'stroke', 'ischemia', 'ischemic', 'infarction',\n",
        "        'stent', 'angioplasty', 'bypass', 'valve', 'mitral', 'tricuspid', 'pulmonary hypertension',\n",
        "        'congestive heart failure', 'chf', 'murmur', 'pulse', 'blood pressure', 'circulation',\n",
        "        'vascular', 'vasculitis', 'chest pain', 'dyspnea', 'syncope', 'edema'\n",
        "    ]\n",
        "\n",
        "    # Check if any keyword is in the question\n",
        "    question_lower = example['question'].lower()\n",
        "    for keyword in heart_keywords:\n",
        "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', question_lower):\n",
        "            return True\n",
        "\n",
        "    # Also check in the metamap_phrases if available\n",
        "    if 'metamap_phrases' in example:\n",
        "        phrases = ' '.join(example['metamap_phrases']).lower()\n",
        "        for keyword in heart_keywords:\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', phrases):\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Filter the dataset to only include heart-related questions\n",
        "print(\"Filtering dataset for heart-related content...\")\n",
        "\n",
        "# Apply the filter\n",
        "train_heart_filtered = dataset[\"train\"].filter(is_heart_related)\n",
        "test_heart_filtered = dataset[\"test\"].filter(is_heart_related)\n",
        "\n",
        "# Create a new dataset dictionary with only heart-related questions\n",
        "heart_dataset = {\"train\": train_heart_filtered, \"test\": test_heart_filtered}\n",
        "print(f\"Heart dataset size: Train={len(heart_dataset['train'])}, Test={len(heart_dataset['test'])}\")\n",
        "\n",
        "# If the heart dataset is too small, augment with some general questions\n",
        "min_training_size = 1000\n",
        "if len(heart_dataset['train']) < min_training_size:\n",
        "    num_additional = min_training_size - len(heart_dataset['train'])\n",
        "    print(f\"Heart dataset is small, adding {num_additional} general questions...\")\n",
        "    # Add some general questions to ensure enough training data\n",
        "    non_heart_dataset = dataset[\"train\"].filter(lambda x: not is_heart_related(x))\n",
        "    additional_examples = non_heart_dataset.select(range(min(num_additional, len(non_heart_dataset))))\n",
        "    # Combine the datasets\n",
        "    heart_dataset[\"train\"] = concatenate_datasets([heart_dataset[\"train\"], additional_examples])\n",
        "    print(f\"Augmented dataset size: Train={len(heart_dataset['train'])}\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"Checking GPU availability...\")\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        test_tensor = torch.tensor([1.0, 2.0, 3.0], device=\"cuda\")\n",
        "        print(\"GPU test successful:\", test_tensor.device)\n",
        "        device = torch.device(\"cuda\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU error: {e}\")\n",
        "        print(\"Falling back to CPU\")\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "print(\"Loading model and tokenizer...\")\n",
        "model_name = \"t5-base\"  # Using t5-base for better performance\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model with proper error handling\n",
        "try:\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    print(\"Model loaded and moved to device successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Trying with CPU only...\")\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "\n",
        "# Format the dataset for T5 with cardiology prefix\n",
        "print(\"Formatting dataset...\")\n",
        "def format_cardio_medqa(example):\n",
        "    # Add a cardiology-specific prefix\n",
        "    input_text = f\"answer cardiology medical question: {example['question']} Options: \"\n",
        "    for key, value in example['options'].items():\n",
        "        input_text += f\"{key}: {value}. \"\n",
        "\n",
        "    # The target is the letter answer (A, B, C, D)\n",
        "    target_text = example['answer_idx']\n",
        "\n",
        "    return {\n",
        "        \"input_text\": input_text,\n",
        "        \"target_text\": target_text\n",
        "    }\n",
        "\n",
        "# Apply formatting to dataset\n",
        "formatted_dataset = {}\n",
        "for split, data in heart_dataset.items():\n",
        "    formatted_dataset[split] = data.map(format_cardio_medqa)\n",
        "\n",
        "# Print a sample formatted entry\n",
        "if len(formatted_dataset[\"train\"]) > 0:\n",
        "    print(\"\\nSample formatted entry:\")\n",
        "    print(formatted_dataset[\"train\"][0])\n",
        "else:\n",
        "    print(\"\\nNo heart-related questions found in the dataset.\")\n",
        "\n",
        "# Tokenize the dataset properly for T5\n",
        "print(\"Tokenizing dataset...\")\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples[\"input_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize the targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"target_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=8,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    # Replace padding token id's with -100 so they're not included in loss computation\n",
        "    labels_with_ignore = labels[\"input_ids\"].clone()\n",
        "    labels_with_ignore[labels_with_ignore == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    inputs[\"labels\"] = labels_with_ignore\n",
        "    return inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = {}\n",
        "for split, data in formatted_dataset.items():\n",
        "    tokenized_dataset[split] = data.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        batch_size=8,\n",
        "        remove_columns=formatted_dataset[split].column_names\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenized dataset structure:\")\n",
        "for split, data in tokenized_dataset.items():\n",
        "    print(f\"{split}: {data}\")\n",
        "\n",
        "# Check if we actually have enough data to train\n",
        "if len(tokenized_dataset[\"train\"]) < 100:\n",
        "    print(\"Not enough heart-related data to train effectively. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Create data collator with padding\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# Define training arguments with more epochs for specialized knowledge\n",
        "print(\"Setting up training configuration...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./heart_model_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=10,              # More epochs for specialized knowledge\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "print(\"Initializing trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Free up memory before training\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Start training with error handling\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./heart-medqa-t5-model\")\n",
        "    tokenizer.save_pretrained(\"./heart-medqa-t5-model\")\n",
        "\n",
        "    # Test the model on a sample heart question\n",
        "    def generate_answer(question, options):\n",
        "        input_text = f\"answer cardiology medical question: {question} Options: \"\n",
        "        for key, value in options.items():\n",
        "            input_text += f\"{key}: {value}. \"\n",
        "\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(**inputs, max_length=8)\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    # Find a heart-related example from the test set\n",
        "    if len(heart_dataset[\"test\"]) > 0:\n",
        "        sample = heart_dataset[\"test\"][0]\n",
        "        print(\"\\nSample heart question:\")\n",
        "        print(sample[\"question\"])\n",
        "        print(\"Options:\", sample[\"options\"])\n",
        "        print(\"Correct answer:\", sample[\"answer_idx\"])\n",
        "\n",
        "        predicted = generate_answer(sample[\"question\"], sample[\"options\"])\n",
        "        print(\"Model prediction:\", predicted)\n",
        "    else:\n",
        "        print(\"No heart-related questions found in the test set for evaluation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    print(\"Troubleshooting recommendations:\")\n",
        "    print(\"1. Try using a smaller model like t5-small\")\n",
        "    print(\"2. Further reduce batch size\")\n",
        "    print(\"3. Consider training on CPU if GPU memory is limited\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5vLKt6wZCYhq",
        "outputId": "2fc358b6-bab9-479d-e482-657f3990c7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Original dataset size: Train=10178, Test=1273\n",
            "Filtering dataset for heart-related content...\n",
            "Heart dataset size: Train=5306, Test=674\n",
            "Checking GPU availability...\n",
            "GPU test successful: cuda:0\n",
            "Using device: cuda\n",
            "Loading model and tokenizer...\n",
            "Model loaded and moved to device successfully\n",
            "Formatting dataset...\n",
            "\n",
            "Sample formatted entry:\n",
            "{'question': 'A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7Â°F (36.5Â°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?', 'answer': 'Nitrofurantoin', 'options': {'A': 'Ampicillin', 'B': 'Ceftriaxone', 'C': 'Doxycycline', 'D': 'Nitrofurantoin'}, 'meta_info': 'step2&3', 'answer_idx': 'D', 'metamap_phrases': ['23 year old pregnant woman', 'weeks presents', 'burning', 'urination', 'states', 'started 1 day', 'worsening', 'drinking', 'water', 'taking cranberry extract', 'feels well', 'followed by', 'doctor', 'pregnancy', 'temperature', '97', '36', 'blood pressure', 'mmHg', 'pulse', '80 min', 'respirations', 'min', 'oxygen saturation', '98', 'room air', 'Physical exam', 'notable', 'absence', 'costovertebral angle tenderness', 'gravid uterus', 'following', 'best treatment', 'patient'], 'input_text': 'answer cardiology medical question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7Â°F (36.5Â°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? Options: A: Ampicillin. B: Ceftriaxone. C: Doxycycline. D: Nitrofurantoin. ', 'target_text': 'D'}\n",
            "Tokenizing dataset...\n",
            "\n",
            "Tokenized dataset structure:\n",
            "train: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5306\n",
            "})\n",
            "test: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 674\n",
            "})\n",
            "Setting up training configuration...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-2-43acd88038dd>:214: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13270' max='13270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13270/13270 1:01:10, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.713500</td>\n",
              "      <td>0.725118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.720400</td>\n",
              "      <td>0.700507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.708900</td>\n",
              "      <td>0.687140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.707000</td>\n",
              "      <td>0.700796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.727100</td>\n",
              "      <td>0.711328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.762700</td>\n",
              "      <td>0.707684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.758900</td>\n",
              "      <td>0.706322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.772300</td>\n",
              "      <td>0.705032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.755200</td>\n",
              "      <td>0.705018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.753900</td>\n",
              "      <td>0.705054</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed successfully!\n",
            "\n",
            "Sample heart question:\n",
            "Two weeks after undergoing an emergency cardiac catherization with stenting for unstable angina pectoris, a 61-year-old man has decreased urinary output and malaise. He has type 2 diabetes mellitus and osteoarthritis of the hips. Prior to admission, his medications were insulin and naproxen. He was also started on aspirin, clopidogrel, and metoprolol after the coronary intervention. His temperature is 38Â°C (100.4Â°F), pulse is 93/min, and blood pressure is 125/85 mm Hg. Examination shows mottled, reticulated purplish discoloration of the feet. Laboratory studies show:\n",
            "Hemoglobin count 14 g/dL\n",
            "Leukocyte count 16,400/mm3\n",
            "Segmented neutrophils 56%\n",
            "Eosinophils 11%\n",
            "Lymphocytes 31%\n",
            "Monocytes 2%\n",
            "Platelet count 260,000/mm3\n",
            "Erythrocyte sedimentation rate 68 mm/h\n",
            "Serum\n",
            "Urea nitrogen 25 mg/dL\n",
            "Creatinine 4.2 mg/dL\n",
            "Renal biopsy shows intravascular spindle-shaped vacuoles. Which of the following is the most likely cause of this patient's symptoms?\"\n",
            "Options: {'A': 'Renal papillary necrosis', 'B': 'Cholesterol embolization', 'C': 'Eosinophilic granulomatosis with polyangiitis', 'D': 'Polyarteritis nodosa'}\n",
            "Correct answer: B\n",
            "Model prediction: C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./heart-medqa-t5-model\")\n",
        "tokenizer.save_pretrained(\"./heart-medqa-t5-model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPbNKqhWG8vK",
        "outputId": "96bca0ea-cdae-4427-a3b4-59ba64cde667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./heart-medqa-t5-model/tokenizer_config.json',\n",
              " './heart-medqa-t5-model/special_tokens_map.json',\n",
              " './heart-medqa-t5-model/spiece.model',\n",
              " './heart-medqa-t5-model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQWhOt0HVki5",
        "outputId": "771ef9ee-557a-4f20-ad4b-c5c3f4abffcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./heart-medqa-t5-model\"  # Path to your saved model\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Additional knowledge base for common cardiology responses\n",
        "cardio_knowledge = {\n",
        "    \"heart attack\": \"A heart attack (myocardial infarction) occurs when blood flow to part of the heart is blocked, causing damage to heart muscle. Common symptoms include chest pain, shortness of breath, and pain radiating to the arm, jaw or back.\",\n",
        "    \"arrhythmia\": \"Arrhythmias are irregular heartbeats caused by problems with the heart's electrical system. They may feel like palpitations, fluttering, or a racing heartbeat.\",\n",
        "    \"hypertension\": \"Hypertension (high blood pressure) is a condition where the force of blood against artery walls is consistently too high. It's often called a 'silent killer' because it frequently has no symptoms.\",\n",
        "    \"murmur\": \"Heart murmurs are unusual sounds heard during a heartbeat cycle, like whooshing or swishing noises. Some are harmless (innocent), while others indicate heart problems.\",\n",
        "    \"ecg\": \"An electrocardiogram (ECG or EKG) records the electrical signals in your heart. It's used to detect heart problems and monitor heart health.\",\n",
        "}\n",
        "\n",
        "def answer_cardio_question(question):\n",
        "    \"\"\"Generate a response to an open-ended cardiology question\"\"\"\n",
        "\n",
        "    # Check if question is empty\n",
        "    if not question.strip():\n",
        "        return \"Please enter a cardiology-related question.\"\n",
        "\n",
        "    # Check if the question matches our knowledge base first\n",
        "    for key, response in cardio_knowledge.items():\n",
        "        if key in question.lower():\n",
        "            return response\n",
        "\n",
        "    # For other questions, we'll modify our approach with the T5 model\n",
        "    # Since the model was trained for multiple choice, we'll create generic options\n",
        "    generic_options = {\n",
        "        \"A\": \"This represents a normal cardiac finding.\",\n",
        "        \"B\": \"This indicates a pathological cardiac condition.\",\n",
        "        \"C\": \"This requires immediate medical intervention.\",\n",
        "        \"D\": \"This is a benign variation requiring monitoring.\"\n",
        "    }\n",
        "\n",
        "    # Format input as the model expects, but with our generic options\n",
        "    input_text = f\"answer cardiology medical question: {question} Options: \"\n",
        "    for key, value in generic_options.items():\n",
        "        input_text += f\"{key}: {value}. \"\n",
        "\n",
        "    # Generate the answer\n",
        "    try:\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(**inputs, max_length=8)\n",
        "        answer_idx = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Based on which option the model chose, provide a relevant response\n",
        "        if answer_idx == \"A\":\n",
        "            response = \"This appears to be a normal cardiac finding. Regular monitoring is advised.\"\n",
        "        elif answer_idx == \"B\":\n",
        "            response = \"This likely indicates a pathological cardiac condition. A consultation with a cardiologist is recommended.\"\n",
        "        elif answer_idx == \"C\":\n",
        "            response = \"This condition may require prompt medical attention. Please consult with a healthcare provider soon.\"\n",
        "        elif answer_idx == \"D\":\n",
        "            response = \"This appears to be a benign variation, but should be monitored by a healthcare professional.\"\n",
        "        else:\n",
        "            response = \"I'm not able to provide a specific answer to this question. Please consult with a healthcare professional.\"\n",
        "\n",
        "        # Add medical disclaimer\n",
        "        return response + \"\\n\\nNote: This information is provided by an AI assistant and should not replace professional medical advice.\"\n",
        "    except Exception as e:\n",
        "        return f\"I encountered an error while processing your question. Please try rephrasing it or consult with a healthcare professional.\"\n",
        "\n",
        "# Create the chat interface with a simpler approach\n",
        "with gr.Blocks(title=\"Cardiology Medical Chatbot\") as demo:\n",
        "    gr.Markdown(\"# Cardiology Medical Assistant\")\n",
        "    gr.Markdown(\"\"\"\n",
        "    This chatbot can answer questions about heart health and cardiology.\n",
        "\n",
        "    **Example questions you can ask:**\n",
        "    - What are the symptoms of a heart attack?\n",
        "    - What is atrial fibrillation?\n",
        "    - How is heart failure treated?\n",
        "    - What does an elevated troponin level indicate?\n",
        "    - What are the risk factors for coronary artery disease?\n",
        "\n",
        "    **Disclaimer:** This AI assistant provides general information only and should not replace professional medical advice.\n",
        "    \"\"\")\n",
        "\n",
        "    # Create a simple chatbot interface\n",
        "    chatbot = gr.Chatbot()\n",
        "\n",
        "    # Input area with submit button\n",
        "    with gr.Row():\n",
        "        input_text = gr.Textbox(\n",
        "            placeholder=\"Enter your cardiology question...\",\n",
        "            lines=2,\n",
        "            scale=4\n",
        "        )\n",
        "        submit_button = gr.Button(\"Submit\", scale=1)\n",
        "\n",
        "    # Clear chat button\n",
        "    clear_button = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # Define the chat function\n",
        "    def chat(user_message, history):\n",
        "        bot_response = answer_cardio_question(user_message)\n",
        "        history.append((user_message, bot_response))\n",
        "        return \"\", history\n",
        "\n",
        "    # Connect components\n",
        "    submit_button.click(\n",
        "        chat,\n",
        "        inputs=[input_text, chatbot],\n",
        "        outputs=[input_text, chatbot]\n",
        "    )\n",
        "\n",
        "    input_text.submit(\n",
        "        chat,\n",
        "        inputs=[input_text, chatbot],\n",
        "        outputs=[input_text, chatbot]\n",
        "    )\n",
        "\n",
        "    # Clear chat history\n",
        "    def clear_history():\n",
        "        return []\n",
        "\n",
        "    clear_button.click(\n",
        "        clear_history,\n",
        "        outputs=[chatbot]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "-wufeT1R_ivM",
        "outputId": "f40172b7-ffc5-4819-bb7d-5b52f51c5c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-f12f179abc5b>:89: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://86bc41644d06ee0c6f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://86bc41644d06ee0c6f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNohvd4h_nlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}